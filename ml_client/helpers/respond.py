"""
Module for initializing LLM and using it to respond to user input.
"""

import ctypes
from concurrent.futures import ThreadPoolExecutor
from llama_cpp import llama_log_set
from langchain_community.llms import LlamaCpp

# Constants for model and settings
MODEL_PATH = "./models/Mistral-7B-v0.1.Q3_K_S.gguf"
DEFAULT_TEMPERATURE = 0.1
MAX_TOKENS = 32
STOP_WORDS = ["Q:", "\n"]
ECHO_PROMPT = True
TIMEOUT_SECONDS = 8  # Timeout for LLM operations in seconds

# Define the callback at the module level to persist it
LLM_LOG_CALLBACK = None


def suppress_llama_cpp_logging():
    """
    Suppresses logging from llama.cpp to keep the console output clean.
    """
    global LLM_LOG_CALLBACK  # pylint: disable=global-statement
    try:
        log_callback_type = ctypes.CFUNCTYPE(
            None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p
        )

        def noop_log_callback(_, __, ___):
            """No-op callback to suppress llama.cpp logs."""

        LLM_LOG_CALLBACK = log_callback_type(noop_log_callback)
        llama_log_set(LLM_LOG_CALLBACK, None)
    except (AttributeError, ImportError) as e:
        print(f"Error in llama_log_set or library import: {e}")
    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"Unexpected error while suppressing llama.cpp logging: {e}")


def initialize_llm(model_path: str, temperature: float) -> LlamaCpp:
    """
    Initializes the LlamaCpp model with the specified parameters.

    Args:
        model_path (str): Path to the model file.
        temperature (float): Sampling temperature for the LLM.

    Returns:
        LlamaCpp: The initialized LLM instance.
    """
    try:

        # pylint: disable=not-callable
        llm = LlamaCpp(
            model_path=model_path,
            temperature=temperature,
            verbose=False,
        )

        # Test LLM initialization
        llm.invoke(
            "Q: Hello World A:",
            max_tokens=MAX_TOKENS,
            stop=STOP_WORDS,
            echo=ECHO_PROMPT,
        )
        return llm
    except FileNotFoundError as e:
        print(f"Model file not found: {e}")
    except RuntimeError as e:
        print(f"Runtime error during LLM initialization: {e}")
    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"Unexpected error initializing LLM: {e}")
    return None


def respond_to_user_input(question: str, llm: LlamaCpp) -> str:
    """
    Processes user input and generates a response using the LLM with a timeout.

    Args:
        question (str): The user's question.
        llm (LlamaCpp): The initialized LLM instance.

    Returns:
        str: The response generated by the LLM or an error message if timeout occurs.
    """
    if llm is None:
        return "LLM initialization failed. Please try again."

    def invoke_model():
        return llm.invoke(
            f"Q: {question} A:",
            max_tokens=MAX_TOKENS,
            stop=STOP_WORDS,
            echo=ECHO_PROMPT,
        )

    try:
        with ThreadPoolExecutor() as executor:
            future = executor.submit(invoke_model)
            response = future.result(timeout=TIMEOUT_SECONDS)
        return response
    except TimeoutError:
        return "The request timed out. Please try again with a shorter input."
    except RuntimeError as e:
        print(f"Runtime error during LLM invocation: {e}")
        return "An error occurred while processing your request."
    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"Unexpected error during LLM invocation: {e}")
        return "An unexpected error occurred. Please try again."


def main():
    """
    Main function to initialize the LLM and process user input.
    """
    suppress_llama_cpp_logging()
    llm = initialize_llm(MODEL_PATH, DEFAULT_TEMPERATURE)

    if llm:
        user_question = input("Enter your question: ")
        response = respond_to_user_input(user_question, llm)
        print(f"Response: {response}")
    else:
        print("Failed to initialize the LLM.")


if __name__ == "__main__":
    main()
